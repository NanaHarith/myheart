<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Dearest...</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
    <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
</head>
<body>
    <h1>My Dearest... I want to talk</h1>
    <button id="startButton">Start Listening</button>
    <div id="status" class="ready">Ready</div>
    <div id="speechStatus"></div>
    <div id="result"></div>
    <div id="console"></div>
    <audio id="beepSound" src="{{ url_for('serve_beep') }}"></audio>
    <audio id="responseAudio" controls style="display: none;"></audio>

    <script>
        const socket = io();
        let recognition;
        let isListening = false;
        let audioContext;
        let analyser;
        let microphone;
        let dataArray;
        let isSpeechDetected = false;
        let transcriptionTimeout;

        const consoleDiv = document.getElementById('console');
        const statusDiv = document.getElementById('status');
        const speechStatusDiv = document.getElementById('speechStatus');
        const resultDiv = document.getElementById('result');
        const startButton = document.getElementById('startButton');
        const beepSound = document.getElementById('beepSound');
        const responseAudio = document.getElementById('responseAudio');

        function updateStatus(message, className) {
            statusDiv.textContent = message;
            statusDiv.className = className;
            logMessage(message);
        }

        function logMessage(message) {
            const messageElement = document.createElement('div');
            messageElement.className = 'console-message';
            messageElement.textContent = new Date().toLocaleTimeString() + ': ' + message;
            consoleDiv.appendChild(messageElement);
            consoleDiv.scrollTop = consoleDiv.scrollHeight;
        }

        startButton.addEventListener('click', toggleListening);

        function toggleListening() {
            if (!isListening) {
                startListening();
            } else {
                stopListening();
            }
        }

        function startListening() {
            if ('webkitSpeechRecognition' in window) {
                recognition = new webkitSpeechRecognition();
                recognition.continuous = true;
                recognition.interimResults = false;

                recognition.onstart = () => {
                    isListening = true;
                    updateStatus('Listening for command...', 'listening');
                    startButton.textContent = 'Stop Listening';
                    socket.emit('start_listening');
                };

                recognition.onresult = (event) => {
                    const transcript = event.results[event.results.length - 1][0].transcript;
                    console.log('Recognized:', transcript);
                    socket.emit('transcription', transcript);
                };

                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    logMessage('Speech recognition error: ' + event.error);
                    stopListening();
                };

                recognition.onend = () => {
                    if (isListening) {
                        recognition.start();
                    }
                };
                try {
                    recognition.start();
                    console.log('Recognition attempted to start');
                } catch (error) {
                    console.error('Error starting recognition:', error);
                }
                // Set up audio context for VAD
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                navigator.mediaDevices.getUserMedia({ audio: true })
                    .then(stream => {
                        microphone = audioContext.createMediaStreamSource(stream);
                        microphone.connect(analyser);
                        analyser.fftSize = 2048;
                        dataArray = new Uint8Array(analyser.frequencyBinCount);
                        detectSpeech();
                    })
                    .catch(err => {
                        console.error('Error accessing microphone:', err);
                        logMessage('Error accessing microphone: ' + err);
                    });
            } else {
                console.error('Web Speech API is not supported in this browser.');
                updateStatus('Speech recognition not supported', 'ready');
            }
        }

        function stopListening() {
            if (recognition) {
                recognition.stop();
                isListening = false;
                updateStatus('Ready', 'ready');
                startButton.textContent = 'Start Listening';
            }
            if (audioContext) {
                audioContext.close();
            }
        }

        function detectSpeech() {
            analyser.getByteFrequencyData(dataArray);
            const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
            console.log('Audio average:', average);
            if (average > 50) {  // Adjust this threshold as needed
                const audioBuffer = new Uint8Array(dataArray);
                socket.emit('audio_data', audioBuffer);
            }
            if (isListening) {
                requestAnimationFrame(detectSpeech);
            }
        }


        socket.on('ai_response', function(data) {
            resultDiv.textContent = data.text;
            if (data.is_final) {
                updateStatus('Ready to listen...', 'ready');
            }
            logMessage('AI Response: ' + data.text);
        });

        socket.on('audio_response', function(data) {
            if (data.url) {
                responseAudio.pause();
                responseAudio.currentTime = 0;
                responseAudio.src = data.url.startsWith('/') ? data.url : URL.createObjectURL(new Blob([data.url], { type: 'audio/mpeg' }));
                responseAudio.style.display = 'block';
                responseAudio.play().catch(e => console.error('Error playing audio:', e)).then(() => {
                    responseAudio.onended = () => {
                        socket.emit('audio_finished');
                        updateStatus('Ready to listen...', 'ready');
                    };
                });
            } else {
                console.error('Received null or undefined audio URL');
            }
        });

        socket.on('speech_detected', function(data) {
            speechStatusDiv.textContent = data.detected ? 'Speech detected' : 'No speech detected';
            if (data.detected) {
                if (!isSpeechDetected) {
                    isSpeechDetected = true;
                    recognition.start();
                }
                clearTimeout(transcriptionTimeout);
                transcriptionTimeout = setTimeout(() => {
                    isSpeechDetected = false;
                    recognition.stop();
                }, 2000); // Adjust this timeout as needed
            }
        });

        socket.on('connect', () => logMessage('Connected to server'));
        socket.on('disconnect', () => logMessage('Disconnected from server'));

        window.onload = () => {
            logMessage('Page loaded. Click "Start Listening" to begin.');
        };
    </script>
</body>
</html>
